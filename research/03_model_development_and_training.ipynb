{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "19414fc5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/home/amit/python/Industrial_AI_project/IITM_MLops_titanic_dataset_github_clone/IITM-MLProject-kaggle-Titanic-dataset'"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "%pwd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "8633b28a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/home/amit/python/Industrial_AI_project/IITM_MLops_titanic_dataset_github_clone/IITM-MLProject-kaggle-Titanic-dataset'"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "os.chdir('../')\n",
    "%pwd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "716eab54",
   "metadata": {},
   "outputs": [],
   "source": [
    "from dataclasses import dataclass\n",
    "from pathlib import Path\n",
    "\n",
    "\n",
    "@dataclass(frozen=True)\n",
    "class ModelTrainingConfig:\n",
    "    root_dir: Path\n",
    "    input_data_file: Path\n",
    "    model_file: Path\n",
    "    params_splitratio: list\n",
    "    params_seed: int\n",
    "    params_regParam: list\n",
    "    params_elasticNetParam: list\n",
    "    params_number_of_folds: int\n",
    "    params_sparkSessionTitle: str"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "04db0232",
   "metadata": {},
   "outputs": [],
   "source": [
    "# from Titanic_dataset_analysis import constants as c\n",
    "# from Titanic_dataset_analysis.constants import CONFIG_FILE_PATH, PARAMS_FILE_PATH\n",
    "CONFIG_FILE_PATH = Path(\"config/config.yaml\")\n",
    "PARAMS_FILE_PATH = Path(\"params.yaml\")\n",
    "from Titanic_dataset_analysis.utils.common import read_yaml, create_directories"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b340094",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ConfigurationManager:\n",
    "    def __init__(\n",
    "        self,\n",
    "        config_filepath = CONFIG_FILE_PATH,\n",
    "        params_filepath = PARAMS_FILE_PATH):\n",
    "        print(os.getcwd())\n",
    "        self.config = read_yaml(config_filepath)\n",
    "        self.params = read_yaml(params_filepath)\n",
    "\n",
    "        create_directories([self.config.artifacts_root])\n",
    "\n",
    "\n",
    "    \n",
    "    def model_training_config(self) -> ModelTrainingConfig:\n",
    "        config = self.config.model_training\n",
    "        self.params = self.params\n",
    "\n",
    "        create_directories([config.root_dir])\n",
    "        # print(f\"Params received under: {self.params} and {self.params.splitratio}\")\n",
    "        model_training_config = ModelTrainingConfig(\n",
    "            root_dir=config.root_dir,\n",
    "            input_data_file=config.input_data_file,\n",
    "            model_file=config.model_file,\n",
    "            params_splitratio=self.params.splitratio,\n",
    "            params_seed=self.params.seed,\n",
    "            params_regParam=self.params.regParam,\n",
    "            params_elasticNetParam=self.params.elasticNetParam,\n",
    "            params_number_of_folds=self.params.number_of_folds,\n",
    "            params_sparkSessionTitle=self.params.sparkSessionTitle\n",
    "            \n",
    "        )\n",
    "\n",
    "        return model_training_config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "333aee2a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import findspark\n",
    "from pathlib import Path\n",
    "findspark.init()\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import col, when\n",
    "from pyspark.ml import Pipeline\n",
    "from pyspark.ml.classification import LogisticRegression\n",
    "from pyspark.ml.feature import StringIndexer, OneHotEncoder, VectorAssembler\n",
    "from pyspark.ml.tuning import CrossValidator, ParamGridBuilder\n",
    "from pyspark.ml.evaluation import BinaryClassificationEvaluator\n",
    "from Titanic_dataset_analysis import logger\n",
    "from Titanic_dataset_analysis.utils.common import get_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "d9087603",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ModelTraining:\n",
    "    def __init__(self, config: ModelTrainingConfig):\n",
    "        self.config = config\n",
    "\n",
    "\n",
    "    \n",
    "    def read_file(self):\n",
    "        if not os.path.exists(self.config.input_data_file):\n",
    "            logger.info(f\"File download failed in previous step! Please check the location mentioned : {self.config.input_data_file}\")\n",
    "        else:\n",
    "            logger.info(f\"File already exists of size: {get_size(Path(self.config.input_data_file))}\")  \n",
    "\n",
    "        self.df = pd.read_csv(self.config.input_data_file)\n",
    "        logger.info(f\"Processed Input file read from {self.config.input_data_file}\")\n",
    "    \n",
    "    def model_training_and_save_file(self):\n",
    "        \"\"\"\n",
    "        zip_file_path: str\n",
    "        Extracts the zip file into the data directory\n",
    "        Function returns None\n",
    "        \"\"\"\n",
    "        # print(self.config)\n",
    "        # logger.info(f\"Spark session title is: {self.config.params_sparkSessionTitle}\")\n",
    "        spark = SparkSession.builder.appName(self.config.params_sparkSessionTitle).getOrCreate()\n",
    "        spark.sparkContext.setLogLevel(\"WARN\")\n",
    "        \n",
    "        df = spark.read.csv(str(self.config.input_data_file), header=True, inferSchema=True)\n",
    "        # logger.info(df.show())\n",
    "        # Drop Cabin + Name + Ticket (not useful for ML in our setup)\n",
    "        columns_to_drop = [\"Cabin\", \"Name\", \"Ticket\"]\n",
    "        # Categorical feature processing\n",
    "        categorical_cols = [\"Sex\", \"Embarked\"]\n",
    "        indexed_cols = [c + \"_indexed\" for c in categorical_cols]\n",
    "        encoded_cols = [c + \"_encoded\" for c in categorical_cols]\n",
    "\n",
    "        indexers = [StringIndexer(inputCol=c, outputCol=c + \"_indexed\", handleInvalid=\"keep\") for c in categorical_cols]\n",
    "        encoders = [OneHotEncoder(inputCol=ic, outputCol=ec) for ic, ec in zip(indexed_cols, encoded_cols)]\n",
    "\n",
    "        # Vector Assembler\n",
    "        feature_columns = [\n",
    "            \"Pclass\", \"Age\", \"SibSp\", \"Parch\", \"Fare\",\n",
    "            \"FamilySize\", \"IsAlone\",\n",
    "            \"Sex_encoded\", \"Embarked_encoded\"\n",
    "        ]\n",
    "        assembler = VectorAssembler(inputCols=feature_columns, outputCol=\"features\")\n",
    "\n",
    "        # Logistic Regression model\n",
    "        lr = LogisticRegression(featuresCol=\"features\", labelCol=\"Survived\")\n",
    "        # -------------------------------\n",
    "        # Pipeline\n",
    "        # -------------------------------\n",
    "        pipeline = Pipeline(stages=indexers + encoders + [assembler, lr])\n",
    "\n",
    "        # -------------------------------\n",
    "        # Train/Test split\n",
    "        # -------------------------------\n",
    "        train_data, test_data = df.drop(*columns_to_drop).randomSplit(self.config.params_splitratio, seed=self.config.params_seed)\n",
    "\n",
    "        # -------------------------------\n",
    "        # Param Grid & CrossValidator\n",
    "        # -------------------------------\n",
    "        paramGrid = ParamGridBuilder() \\\n",
    "            .addGrid(lr.regParam, self.config.params_regParam) \\\n",
    "            .addGrid(lr.elasticNetParam, self.config.params_elasticNetParam) \\\n",
    "            .build()\n",
    "\n",
    "        # Evaluator\n",
    "        evaluator = BinaryClassificationEvaluator(labelCol=\"Survived\", rawPredictionCol=\"rawPrediction\", metricName=\"areaUnderROC\")\n",
    "\n",
    "        crossval = CrossValidator(estimator=pipeline,\n",
    "                                estimatorParamMaps=paramGrid,\n",
    "                                evaluator=evaluator,\n",
    "                                numFolds=self.config.params_number_of_folds)\n",
    "        cv_model = crossval.fit(train_data)\n",
    "        best_lr = cv_model.bestModel.stages[-1]\n",
    "        logger.info(f\"Best model: {best_lr}\")\n",
    "        # logger.info(f\"Best regParam: {best_lr.getOrDefault('regParam')}\")\n",
    "        # logger.info(f\"Best elasticNetParam: {best_lr.getOrDefault('elasticNetParam')}\")\n",
    "        \n",
    "        \n",
    "        # # Basic cleaning / feature engineering\n",
    "        # mean_age = self.df['Age'].mean()\n",
    "        # mode_embarked = self.df['Embarked'].mode().iloc[0] if 'Embarked' in self.df.columns else 'S'\n",
    "        # self.df['Age'] = pd.to_numeric(self.df['Age'], errors='coerce').fillna(mean_age)\n",
    "        # if 'Fare' in self.df.columns:\n",
    "        #     self.df['Fare'] = pd.to_numeric(self.df['Fare'], errors='coerce').fillna(0.0)\n",
    "        # self.df['SibSp'] = pd.to_numeric(self.df.get('SibSp', 0), errors='coerce').fillna(0).astype(int)\n",
    "        # self.df['Parch'] = pd.to_numeric(self.df.get('Parch', 0), errors='coerce').fillna(0).astype(int)\n",
    "        # self.df['FamilySize'] = self.df['SibSp'] + self.df['Parch'] + 1\n",
    "        # self.df['IsAlone'] = (self.df['FamilySize'] == 1).astype(int)\n",
    "        # if 'Embarked' in self.df.columns:\n",
    "        #     self.df['Embarked'] = self.df['Embarked'].fillna(mode_embarked)\n",
    "        \n",
    "        os.makedirs(self.config.root_dir, exist_ok=True)\n",
    "        model_path =f\"{self.config.root_dir}/best_model\"\n",
    "        if os.path.exists(model_path):\n",
    "            cv_model.bestModel.write().overwrite().save(model_path)\n",
    "        else:\n",
    "            cv_model.bestModel.write().save(model_path)\n",
    "        # cv_model.bestModel.write().save(f\"{self.config.root_dir}/best_model\") # .overwrite()\n",
    "        logger.info(f\"Model training done successfully. Model saved at {self.config.root_dir}/best_model\")\n",
    "        \n",
    "        spark.stop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "3daf1960",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/amit/python/Industrial_AI_project/IITM_MLops_titanic_dataset_github_clone/IITM-MLProject-kaggle-Titanic-dataset\n",
      "[2025-08-27 18:55:32,455: INFO: common: yaml file: config/config.yaml loaded successfully]\n",
      "[2025-08-27 18:55:32,465: INFO: common: yaml file: params.yaml loaded successfully]\n",
      "[2025-08-27 18:55:32,469: INFO: common: created directory at: artifacts]\n",
      "[2025-08-27 18:55:32,473: INFO: common: created directory at: artifacts/model_training]\n",
      "Params received under: {'splitratio': [0.8, 0.2], 'seed': 42, 'regParam': [0.01, 0.1], 'elasticNetParam': [0.0], 'number_of_folds': 5, 'sparkSessionTitle': 'Titanic_data'} and [0.8, 0.2]\n",
      "[2025-08-27 18:55:32,478: INFO: 3457705151: File already exists of size: ~ 67 KB]\n",
      "[2025-08-27 18:55:32,488: INFO: 3457705151: Processed Input file read from artifacts/data_preprocessing/titanic_preprocessed.csv]\n",
      "[2025-08-27 18:55:50,219: INFO: clientserver: Closing down clientserver connection]\n",
      "[2025-08-27 18:55:50,221: INFO: clientserver: Closing down clientserver connection]\n",
      "[2025-08-27 18:55:50,233: INFO: 3457705151: Best model: LogisticRegressionModel: uid=LogisticRegression_bb5ad27c3982, numClasses=2, numFeatures=12]\n",
      "[2025-08-27 18:55:51,516: INFO: 3457705151: Model training done successfully. Model saved at artifacts/model_training/best_model]\n"
     ]
    }
   ],
   "source": [
    "try:\n",
    "    config = ConfigurationManager()\n",
    "    model_training_config = config.model_training_config()\n",
    "    model_training = ModelTraining(config=model_training_config)\n",
    "    model_training.read_file()\n",
    "    model_training.model_training_and_save_file()\n",
    "except Exception as e:\n",
    "    raise e"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "titanic_end_sem",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.20"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
