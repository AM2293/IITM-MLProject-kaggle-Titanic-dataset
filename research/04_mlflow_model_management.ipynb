{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "19414fc5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/home/amit/python/Industrial_AI_project/IITM_MLops_titanic_dataset_github_clone/IITM-MLProject-kaggle-Titanic-dataset'"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "%pwd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "8633b28a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/home/amit/python/Industrial_AI_project/IITM_MLops_titanic_dataset_github_clone/IITM-MLProject-kaggle-Titanic-dataset'"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "os.chdir('../')\n",
    "%pwd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "716eab54",
   "metadata": {},
   "outputs": [],
   "source": [
    "from dataclasses import dataclass\n",
    "from pathlib import Path\n",
    "\n",
    "\n",
    "@dataclass(frozen=True)\n",
    "class MLFlowModelManagementConfig:\n",
    "    root_dir: Path\n",
    "    input_model_folder: Path\n",
    "    test_data_file: Path\n",
    "    params_experiment_name: str\n",
    "    params_mlflow_uri: str\n",
    "    params_mlflow_run_name: str\n",
    "    params_sparkSessionTitle: str"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "04db0232",
   "metadata": {},
   "outputs": [],
   "source": [
    "# from Titanic_dataset_analysis import constants as c\n",
    "# from Titanic_dataset_analysis.constants import CONFIG_FILE_PATH, PARAMS_FILE_PATH\n",
    "CONFIG_FILE_PATH = Path(\"config/config.yaml\")\n",
    "PARAMS_FILE_PATH = Path(\"params.yaml\")\n",
    "from Titanic_dataset_analysis.utils.common import read_yaml, create_directories"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "5b340094",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ConfigurationManager:\n",
    "    def __init__(\n",
    "        self,\n",
    "        config_filepath = CONFIG_FILE_PATH,\n",
    "        params_filepath = PARAMS_FILE_PATH):\n",
    "        print(os.getcwd())\n",
    "        self.config = read_yaml(config_filepath)\n",
    "        self.params = read_yaml(params_filepath)\n",
    "\n",
    "        create_directories([self.config.artifacts_root])\n",
    "\n",
    "\n",
    "    \n",
    "    def mlflow_model_management_config(self) -> MLFlowModelManagementConfig:\n",
    "        config = self.config.mlflow_model_management\n",
    "        self.params = self.params\n",
    "\n",
    "        create_directories([config.root_dir])\n",
    "        # print(f\"Params received under: {self.params} and {self.params.splitratio}\")\n",
    "        mlflow_model_management_config = MLFlowModelManagementConfig(\n",
    "            root_dir=config.root_dir,\n",
    "            input_model_folder= config.input_model_folder,\n",
    "            test_data_file= config.test_data_file,\n",
    "            params_experiment_name= self.params.experiment_name,\n",
    "            params_mlflow_uri= self.params.mlflow_uri,\n",
    "            params_mlflow_run_name= self.params.mlflow_run_name,\n",
    "            params_sparkSessionTitle= self.params.sparkSessionTitle\n",
    "            \n",
    "        )\n",
    "\n",
    "        return mlflow_model_management_config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "333aee2a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import mlflow\n",
    "from pyspark.sql import SparkSession\n",
    "from pathlib import Path\n",
    "from pyspark.ml import PipelineModel\n",
    "from Titanic_dataset_analysis import logger\n",
    "from pyspark.ml.evaluation import BinaryClassificationEvaluator, MulticlassClassificationEvaluator\n",
    "from Titanic_dataset_analysis import constants as const"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d9087603",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MLFlowModelManagement:\n",
    "    def __init__(self, config: MLFlowModelManagementConfig):\n",
    "        self.config = config\n",
    "\n",
    "\n",
    "    \n",
    "    def load_model(self):\n",
    "        if not os.path.exists(self.config.input_model_folder):\n",
    "            logger.info(f\"Model download failed in previous step! Please check the location mentioned : {self.config.input_data_file}\")\n",
    "        else:\n",
    "            logger.info(f\"Model already exists at: {Path(self.config.input_model_folder)}\")  \n",
    "\n",
    "        self.model = PipelineModel.load(self.config.input_model_folder)\n",
    "        logger.info(f\"Loaded model from {self.config.input_model_folder}\")\n",
    "    \n",
    "    def mlflow_model_tracking(self):\n",
    "        \"\"\"\n",
    "        zip_file_path: str\n",
    "        Extracts the zip file into the data directory\n",
    "        Function returns None\n",
    "        \"\"\"\n",
    "        # -------------------------------\n",
    "        # MLflow Tracking\n",
    "        # -------------------------------\n",
    "        spark = SparkSession.builder.appName(self.config.params_sparkSessionTitle).getOrCreate()\n",
    "        spark.sparkContext.setLogLevel(\"WARN\")\n",
    "        \n",
    "        df = spark.read.csv(str(self.config.test_data_file), header=True, inferSchema=True)\n",
    "        # # Family size + IsAlone\n",
    "        # df = df.withColumn(\"FamilySize\", col(\"SibSp\") + col(\"Parch\") + 1)\n",
    "        # df = df.withColumn(\"IsAlone\", when(col(\"FamilySize\") == 1, 1).otherwise(0))\n",
    "        logger.info(f\"Experiment Name: {self.config.params_experiment_name}\")\n",
    "        experiment_name = self.config.params_experiment_name\n",
    "        logger.info(f\"Setting Tracking URI to : {self.config.params_mlflow_uri}\")\n",
    "        mlflow.set_tracking_uri(self.config.params_mlflow_uri)\n",
    "        os.environ[\"MLFLOW_ARTIFACT_URI\"] = f\"file:{os.getcwd()}/mlruns\"\n",
    "        try:\n",
    "            experiment_id = mlflow.create_experiment(experiment_name)\n",
    "        except mlflow.exceptions.MlflowException:\n",
    "            experiment_id = mlflow.get_experiment_by_name(experiment_name).experiment_id\n",
    "        mlflow.set_experiment(experiment_name)\n",
    "        logger.info(f\"Experiment ID: {experiment_id}\")\n",
    "        with mlflow.start_run(run_name=self.config.params_mlflow_run_name) as run:\n",
    "            # cv_model = crossval.fit(train_data)\n",
    "    \n",
    "            # Best params from final stage (LogReg is last in pipeline)\n",
    "            best_lr = self.model.stages[-1]\n",
    "            # print(best_lr)\n",
    "            logger.info(f\"regparam: {best_lr.getOrDefault('regParam')}\")\n",
    "            logger.info(f\"ElasticNetParam: {best_lr.getOrDefault('elasticNetParam')}\")\n",
    "            mlflow.log_param(\"regParam\", best_lr.getOrDefault(\"regParam\"))\n",
    "            mlflow.log_param(\"elasticNetParam\", best_lr.getOrDefault(\"elasticNetParam\"))\n",
    "    \n",
    "            predictions = self.model.transform(df)\n",
    "            \n",
    "            # Evaluator\n",
    "            evaluator = BinaryClassificationEvaluator(labelCol=\"Survived\", rawPredictionCol=\"rawPrediction\", metricName=\"areaUnderROC\")\n",
    "            # Metrics\n",
    "            auc_test = evaluator.evaluate(predictions)\n",
    "            mlflow.log_metric(\"AUC\", auc_test)\n",
    "            logger.info(f\"AUC: {auc_test}\")\n",
    "\n",
    "            accuracy = MulticlassClassificationEvaluator(labelCol=\"Survived\", predictionCol=\"prediction\", metricName=\"accuracy\").evaluate(predictions)\n",
    "            precision = MulticlassClassificationEvaluator(labelCol=\"Survived\", predictionCol=\"prediction\", metricName=\"weightedPrecision\").evaluate(predictions)\n",
    "            recall = MulticlassClassificationEvaluator(labelCol=\"Survived\", predictionCol=\"prediction\", metricName=\"weightedRecall\").evaluate(predictions)\n",
    "            f1 = MulticlassClassificationEvaluator(labelCol=\"Survived\", predictionCol=\"prediction\", metricName=\"f1\").evaluate(predictions)\n",
    "\n",
    "            mlflow.log_metric(\"Accuracy\", accuracy)\n",
    "            mlflow.log_metric(\"Precision\", precision)\n",
    "            mlflow.log_metric(\"Recall\", recall)\n",
    "            mlflow.log_metric(\"F1\", f1)\n",
    "            logger.info(f\"Accuracy: {accuracy}\")\n",
    "            logger.info(f\"Precision: {precision}\")\n",
    "            logger.info(f\"Recall: {recall}\")\n",
    "            logger.info(f\"F1: {f1}\")\n",
    "            \n",
    "            \n",
    "            # model_name = \"spark-titanic-LR-pipeline\"\n",
    "            mlflow.spark.log_model(self.model,\n",
    "                           artifact_path=self.config.params_mlflow_run_name,\n",
    "                           registered_model_name=self.config.params_experiment_name)\n",
    "\n",
    "            # print(f\"Run ID: {run.info.run_id}\")\n",
    "            # print(f\"Experiment ID: {run.info.experiment_id}\")\n",
    "            const.RUN_ID = run.info.run_id\n",
    "            const.EXPERIMENT_ID = run.info.experiment_id\n",
    "\n",
    "            logger.info(f\"Run ID from constants: {run.info.run_id}\")\n",
    "            logger.info(f\"Experiment ID from constants: {run.info.experiment_id}\")\n",
    "                \n",
    "        os.makedirs(self.config.root_dir, exist_ok=True)\n",
    "        logger.info(f\"MLFlow Model Tracking done successfully.\")\n",
    "        client = mlflow.tracking.MlflowClient()\n",
    "        latest_versions = client.get_latest_versions(self.config.params_experiment_name, stages=[\"None\"])\n",
    "        if latest_versions:\n",
    "            latest_version = latest_versions[0].version\n",
    "            client.transition_model_version_stage(\n",
    "                name=self.config.params_experiment_name,\n",
    "                version=latest_version,\n",
    "                stage=\"Staging\"\n",
    "            )\n",
    "            print(f\"Model '{self.config.params_experiment_name}' version {latest_version} moved to Staging!\")\n",
    "        spark.stop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "3daf1960",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/amit/python/Industrial_AI_project/IITM_MLops_titanic_dataset_github_clone/IITM-MLProject-kaggle-Titanic-dataset\n",
      "[2025-08-28 13:13:06,225: INFO: common: yaml file: config/config.yaml loaded successfully]\n",
      "[2025-08-28 13:13:06,230: INFO: common: yaml file: params.yaml loaded successfully]\n",
      "[2025-08-28 13:13:06,234: INFO: common: created directory at: artifacts]\n",
      "[2025-08-28 13:13:06,236: INFO: common: created directory at: artifacts/mlflow_model_management]\n",
      "[2025-08-28 13:13:06,239: INFO: 104836984: Model already exists at: artifacts/model_training/best_model]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2025-08-28 13:13:10,353: INFO: 104836984: Loaded model from artifacts/model_training/best_model]\n",
      "[2025-08-28 13:13:10,675: INFO: 104836984: Experiment Name: Titanic_Pipeline_Exp1]\n",
      "[2025-08-28 13:13:10,678: INFO: 104836984: Setting Tracking URI to : http://localhost:5000]\n",
      "[2025-08-28 13:13:10,797: INFO: 104836984: Experiment ID: 285969590943178826]\n",
      "[2025-08-28 13:13:10,862: INFO: 104836984: regparam: 0.01]\n",
      "[2025-08-28 13:13:10,863: INFO: 104836984: ElasticNetParam: 0.0]\n",
      "[2025-08-28 13:13:13,023: INFO: 104836984: AUC: 0.8857634902411028]\n",
      "[2025-08-28 13:13:14,158: INFO: 104836984: Accuracy: 0.7862068965517242]\n",
      "[2025-08-28 13:13:14,159: INFO: 104836984: Precision: 0.7887039239001188]\n",
      "[2025-08-28 13:13:14,162: INFO: 104836984: Recall: 0.7862068965517242]\n",
      "[2025-08-28 13:13:14,167: INFO: 104836984: F1: 0.784341065830721]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "25/08/28 13:13:14 ERROR Instrumentation: org.apache.hadoop.fs.UnsupportedFileSystemException: No FileSystem for scheme \"mlflow-artifacts\"\n",
      "\tat org.apache.hadoop.fs.FileSystem.getFileSystemClass(FileSystem.java:3443)\n",
      "\tat org.apache.hadoop.fs.FileSystem.createFileSystem(FileSystem.java:3466)\n",
      "\tat org.apache.hadoop.fs.FileSystem.access$300(FileSystem.java:174)\n",
      "\tat org.apache.hadoop.fs.FileSystem$Cache.getInternal(FileSystem.java:3574)\n",
      "\tat org.apache.hadoop.fs.FileSystem$Cache.get(FileSystem.java:3521)\n",
      "\tat org.apache.hadoop.fs.FileSystem.get(FileSystem.java:540)\n",
      "\tat org.apache.hadoop.fs.Path.getFileSystem(Path.java:365)\n",
      "\tat org.apache.spark.ml.util.FileSystemOverwrite.handleOverwrite(ReadWrite.scala:673)\n",
      "\tat org.apache.spark.ml.util.MLWriter.save(ReadWrite.scala:167)\n",
      "\tat org.apache.spark.ml.PipelineModel$PipelineModelWriter.super$save(Pipeline.scala:344)\n",
      "\tat org.apache.spark.ml.PipelineModel$PipelineModelWriter.$anonfun$save$4(Pipeline.scala:344)\n",
      "\tat org.apache.spark.ml.MLEvents.withSaveInstanceEvent(events.scala:174)\n",
      "\tat org.apache.spark.ml.MLEvents.withSaveInstanceEvent$(events.scala:169)\n",
      "\tat org.apache.spark.ml.util.Instrumentation.withSaveInstanceEvent(Instrumentation.scala:42)\n",
      "\tat org.apache.spark.ml.PipelineModel$PipelineModelWriter.$anonfun$save$3(Pipeline.scala:344)\n",
      "\tat org.apache.spark.ml.PipelineModel$PipelineModelWriter.$anonfun$save$3$adapted(Pipeline.scala:344)\n",
      "\tat org.apache.spark.ml.util.Instrumentation$.$anonfun$instrumented$1(Instrumentation.scala:191)\n",
      "\tat scala.util.Try$.apply(Try.scala:213)\n",
      "\tat org.apache.spark.ml.util.Instrumentation$.instrumented(Instrumentation.scala:191)\n",
      "\tat org.apache.spark.ml.PipelineModel$PipelineModelWriter.save(Pipeline.scala:344)\n",
      "\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n",
      "\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\n",
      "\tat java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n",
      "\tat java.base/java.lang.reflect.Method.invoke(Method.java:566)\n",
      "\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n",
      "\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:374)\n",
      "\tat py4j.Gateway.invoke(Gateway.java:282)\n",
      "\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n",
      "\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n",
      "\tat py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)\n",
      "\tat py4j.ClientServerConnection.run(ClientServerConnection.java:106)\n",
      "\tat java.base/java.lang.Thread.run(Thread.java:829)\n",
      "\n",
      "2025/08/28 13:13:47 WARNING mlflow.models.model: Model logged without a signature and input example. Please set `input_example` parameter when logging the model to auto infer the model signature.\n",
      "Registered model 'Titanic_Pipeline_Exp1' already exists. Creating a new version of this model...\n",
      "2025/08/28 13:13:51 INFO mlflow.store.model_registry.abstract_store: Waiting up to 300 seconds for model version to finish creation. Model name: Titanic_Pipeline_Exp1, version 5\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2025-08-28 13:13:51,340: INFO: 104836984: Run ID from constants: 7ce974ca0d754096a1fe7b209fa0656f]\n",
      "[2025-08-28 13:13:51,342: INFO: 104836984: Experiment ID from constants: 285969590943178826]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Created version '5' of model 'Titanic_Pipeline_Exp1'.\n",
      "2025/08/28 13:13:51 INFO mlflow.tracking._tracking_service.client: üèÉ View run LR_Pipeline at: http://localhost:5000/#/experiments/285969590943178826/runs/7ce974ca0d754096a1fe7b209fa0656f.\n",
      "2025/08/28 13:13:51 INFO mlflow.tracking._tracking_service.client: üß™ View experiment at: http://localhost:5000/#/experiments/285969590943178826.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2025-08-28 13:13:51,424: INFO: 104836984: MLFlow Model Tracking done successfully.]\n"
     ]
    }
   ],
   "source": [
    "try:\n",
    "    config = ConfigurationManager()\n",
    "    mlflow_model_tracking_config = config.mlflow_model_management_config()\n",
    "    mlflow_model_tracking_config = MLFlowModelManagement(config=mlflow_model_tracking_config)\n",
    "    mlflow_model_tracking_config.load_model()\n",
    "    mlflow_model_tracking_config.mlflow_model_tracking()\n",
    "except Exception as e:\n",
    "    raise e"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "9d307361",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/home/amit/python/Industrial_AI_project/IITM_MLops_titanic_dataset_github_clone/IITM-MLProject-kaggle-Titanic-dataset'"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%pwd"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "titanic_end_sem",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.20"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
