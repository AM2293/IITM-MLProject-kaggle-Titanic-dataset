{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "19414fc5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/home/amit/python/Industrial_AI_project/IITM_MLops_titanic_dataset_github_clone/IITM-MLProject-kaggle-Titanic-dataset/research'"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "%pwd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "8633b28a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/home/amit/python/Industrial_AI_project/IITM_MLops_titanic_dataset_github_clone/IITM-MLProject-kaggle-Titanic-dataset'"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "os.chdir('../')\n",
    "%pwd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "716eab54",
   "metadata": {},
   "outputs": [],
   "source": [
    "from dataclasses import dataclass\n",
    "from pathlib import Path\n",
    "\n",
    "\n",
    "@dataclass(frozen=True)\n",
    "class MLFlowModelManagementConfig:\n",
    "    root_dir: Path\n",
    "    input_model_folder: Path\n",
    "    test_data_file: Path\n",
    "    params_experiment_name: str\n",
    "    params_mlflow_uri: str\n",
    "    params_mlflow_run_name: str\n",
    "    params_sparkSessionTitle: str"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "04db0232",
   "metadata": {},
   "outputs": [],
   "source": [
    "# from Titanic_dataset_analysis import constants as c\n",
    "# from Titanic_dataset_analysis.constants import CONFIG_FILE_PATH, PARAMS_FILE_PATH\n",
    "CONFIG_FILE_PATH = Path(\"config/config.yaml\")\n",
    "PARAMS_FILE_PATH = Path(\"params.yaml\")\n",
    "from Titanic_dataset_analysis.utils.common import read_yaml, create_directories"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "5b340094",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ConfigurationManager:\n",
    "    def __init__(\n",
    "        self,\n",
    "        config_filepath = CONFIG_FILE_PATH,\n",
    "        params_filepath = PARAMS_FILE_PATH):\n",
    "        print(os.getcwd())\n",
    "        self.config = read_yaml(config_filepath)\n",
    "        self.params = read_yaml(params_filepath)\n",
    "\n",
    "        create_directories([self.config.artifacts_root])\n",
    "\n",
    "\n",
    "    \n",
    "    def mlflow_model_management_config(self) -> MLFlowModelManagementConfig:\n",
    "        config = self.config.mlflow_model_management\n",
    "        self.params = self.params\n",
    "\n",
    "        create_directories([config.root_dir])\n",
    "        # print(f\"Params received under: {self.params} and {self.params.splitratio}\")\n",
    "        mlflow_model_management_config = MLFlowModelManagementConfig(\n",
    "            root_dir=config.root_dir,\n",
    "            input_model_folder= config.input_model_folder,\n",
    "            test_data_file= config.test_data_file,\n",
    "            params_experiment_name= self.params.experiment_name,\n",
    "            params_mlflow_uri= self.params.mlflow_uri,\n",
    "            params_mlflow_run_name= self.params.mlflow_run_name,\n",
    "            params_sparkSessionTitle= self.params.sparkSessionTitle\n",
    "            \n",
    "        )\n",
    "\n",
    "        return mlflow_model_management_config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "333aee2a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import mlflow\n",
    "from pyspark.sql import SparkSession\n",
    "from pathlib import Path\n",
    "from pyspark.ml import PipelineModel\n",
    "from Titanic_dataset_analysis import logger\n",
    "from pyspark.ml.evaluation import BinaryClassificationEvaluator, MulticlassClassificationEvaluator\n",
    "from Titanic_dataset_analysis import constants as const"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "d9087603",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MLFlowModelManagement:\n",
    "    def __init__(self, config: MLFlowModelManagementConfig):\n",
    "        self.config = config\n",
    "\n",
    "\n",
    "    \n",
    "    def load_model(self):\n",
    "        if not os.path.exists(self.config.input_model_folder):\n",
    "            logger.info(f\"Model download failed in previous step! Please check the location mentioned : {self.config.input_data_file}\")\n",
    "        else:\n",
    "            logger.info(f\"Model already exists at: {Path(self.config.input_model_folder)}\")  \n",
    "\n",
    "        self.model = PipelineModel.load(self.config.input_model_folder)\n",
    "        logger.info(f\"Loaded model from {self.config.input_model_folder}\")\n",
    "    \n",
    "    def mlflow_model_tracking(self):\n",
    "        \"\"\"\n",
    "        zip_file_path: str\n",
    "        Extracts the zip file into the data directory\n",
    "        Function returns None\n",
    "        \"\"\"\n",
    "        # -------------------------------\n",
    "        # MLflow Tracking\n",
    "        # -------------------------------\n",
    "        spark = SparkSession.builder.appName(self.config.params_sparkSessionTitle).getOrCreate()\n",
    "        spark.sparkContext.setLogLevel(\"WARN\")\n",
    "        \n",
    "        df = spark.read.csv(str(self.config.test_data_file), header=True, inferSchema=True)\n",
    "        # # Family size + IsAlone\n",
    "        # df = df.withColumn(\"FamilySize\", col(\"SibSp\") + col(\"Parch\") + 1)\n",
    "        # df = df.withColumn(\"IsAlone\", when(col(\"FamilySize\") == 1, 1).otherwise(0))\n",
    "        logger.info(f\"Experiment Name: {self.config.params_experiment_name}\")\n",
    "        experiment_name = self.config.params_experiment_name\n",
    "        logger.info(f\"Setting Tracking URI to : {self.config.params_mlflow_uri}\")\n",
    "        mlflow.set_tracking_uri(self.config.params_mlflow_uri)\n",
    "        os.environ[\"MLFLOW_ARTIFACT_URI\"] = f\"file:{os.getcwd()}/mlruns\"\n",
    "        try:\n",
    "            experiment_id = mlflow.create_experiment(experiment_name)\n",
    "        except mlflow.exceptions.MlflowException:\n",
    "            experiment_id = mlflow.get_experiment_by_name(experiment_name).experiment_id\n",
    "        mlflow.set_experiment(experiment_name)\n",
    "        logger.info(f\"Experiment ID: {experiment_id}\")\n",
    "        with mlflow.start_run(run_name=self.config.params_mlflow_run_name) as run:\n",
    "            # cv_model = crossval.fit(train_data)\n",
    "    \n",
    "            # Best params from final stage (LogReg is last in pipeline)\n",
    "            best_lr = self.model.stages[-1]\n",
    "            # print(best_lr)\n",
    "            logger.info(f\"regparam: {best_lr.getOrDefault('regParam')}\")\n",
    "            logger.info(f\"ElasticNetParam: {best_lr.getOrDefault('elasticNetParam')}\")\n",
    "            mlflow.log_param(\"regParam\", best_lr.getOrDefault(\"regParam\"))\n",
    "            mlflow.log_param(\"elasticNetParam\", best_lr.getOrDefault(\"elasticNetParam\"))\n",
    "    \n",
    "            predictions = self.model.transform(df)\n",
    "            \n",
    "            # Evaluator\n",
    "            evaluator = BinaryClassificationEvaluator(labelCol=\"Survived\", rawPredictionCol=\"rawPrediction\", metricName=\"areaUnderROC\")\n",
    "            # Metrics\n",
    "            auc_test = evaluator.evaluate(predictions)\n",
    "            mlflow.log_metric(\"AUC\", auc_test)\n",
    "            logger.info(f\"AUC: {auc_test}\")\n",
    "\n",
    "            accuracy = MulticlassClassificationEvaluator(labelCol=\"Survived\", predictionCol=\"prediction\", metricName=\"accuracy\").evaluate(predictions)\n",
    "            precision = MulticlassClassificationEvaluator(labelCol=\"Survived\", predictionCol=\"prediction\", metricName=\"weightedPrecision\").evaluate(predictions)\n",
    "            recall = MulticlassClassificationEvaluator(labelCol=\"Survived\", predictionCol=\"prediction\", metricName=\"weightedRecall\").evaluate(predictions)\n",
    "            f1 = MulticlassClassificationEvaluator(labelCol=\"Survived\", predictionCol=\"prediction\", metricName=\"f1\").evaluate(predictions)\n",
    "\n",
    "            mlflow.log_metric(\"Accuracy\", accuracy)\n",
    "            mlflow.log_metric(\"Precision\", precision)\n",
    "            mlflow.log_metric(\"Recall\", recall)\n",
    "            mlflow.log_metric(\"F1\", f1)\n",
    "            logger.info(f\"Accuracy: {accuracy}\")\n",
    "            logger.info(f\"Precision: {precision}\")\n",
    "            logger.info(f\"Recall: {recall}\")\n",
    "            logger.info(f\"F1: {f1}\")\n",
    "            \n",
    "            \n",
    "            # model_name = \"spark-titanic-LR-pipeline\"\n",
    "            mlflow.spark.log_model(self.model,\n",
    "                           artifact_path=self.config.params_mlflow_run_name,\n",
    "                           registered_model_name=self.config.params_experiment_name)\n",
    "\n",
    "            # print(f\"Run ID: {run.info.run_id}\")\n",
    "            # print(f\"Experiment ID: {run.info.experiment_id}\")\n",
    "            const.RUN_ID = run.info.run_id\n",
    "            const.EXPERIMENT_ID = run.info.experiment_id\n",
    "\n",
    "            logger.info(f\"Run ID from constants: {run.info.run_id}\")\n",
    "            logger.info(f\"Experiment ID from constants: {run.info.experiment_id}\")\n",
    "                \n",
    "        os.makedirs(self.config.root_dir, exist_ok=True)\n",
    "        logger.info(f\"MLFlow Model Tracking done successfully.\")\n",
    "        client = mlflow.tracking.MlflowClient()\n",
    "        latest_versions = client.get_latest_versions(self.config.params_experiment_name, stages=[\"None\"])\n",
    "        if latest_versions:\n",
    "            latest_version = latest_versions[0].version\n",
    "            client.transition_model_version_stage(\n",
    "                name=self.config.params_experiment_name,\n",
    "                version=latest_version,\n",
    "                stage=\"Production\",\n",
    "                archive_existing_versions=True\n",
    "                # stage=\"Staging\"\n",
    "            )\n",
    "            print(f\"Model '{self.config.params_experiment_name}' version {latest_version} moved to Prod not on Staging!\")\n",
    "        spark.stop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "3daf1960",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/amit/python/Industrial_AI_project/IITM_MLops_titanic_dataset_github_clone/IITM-MLProject-kaggle-Titanic-dataset\n",
      "[2025-08-29 08:38:45,117: INFO: common: yaml file: config/config.yaml loaded successfully]\n",
      "[2025-08-29 08:38:45,124: INFO: common: yaml file: params.yaml loaded successfully]\n",
      "[2025-08-29 08:38:45,126: INFO: common: created directory at: artifacts]\n",
      "[2025-08-29 08:38:45,128: INFO: common: created directory at: artifacts/mlflow_model_management]\n",
      "[2025-08-29 08:38:45,130: INFO: 1945385980: Model already exists at: artifacts/model_training/best_model]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "25/08/29 08:38:49 WARN Utils: Your hostname, DESKTOP-VJA6A5N resolves to a loopback address: 127.0.1.1; using 10.255.255.254 instead (on interface lo)\n",
      "25/08/29 08:38:49 WARN Utils: Set SPARK_LOCAL_IP if you need to bind to another address\n",
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n",
      "25/08/29 08:38:50 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2025-08-29 08:39:09,844: INFO: 1945385980: Loaded model from artifacts/model_training/best_model]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "25/08/29 08:39:09 WARN SparkSession: Using an existing Spark session; only runtime SQL configurations will take effect.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2025-08-29 08:39:11,055: INFO: 1945385980: Experiment Name: Titanic_Pipeline_Exp1]\n",
      "[2025-08-29 08:39:11,057: INFO: 1945385980: Setting Tracking URI to : http://localhost:5000]\n",
      "[2025-08-29 08:39:11,420: INFO: 1945385980: Experiment ID: 285969590943178826]\n",
      "[2025-08-29 08:39:11,840: INFO: 1945385980: regparam: 0.1]\n",
      "[2025-08-29 08:39:11,843: INFO: 1945385980: ElasticNetParam: 0.0]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "25/08/29 08:39:14 WARN InstanceBuilder: Failed to load implementation from:dev.ludovic.netlib.blas.JNIBLAS\n",
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2025-08-29 08:39:15,534: INFO: 1945385980: AUC: 0.8763872942977428]\n",
      "[2025-08-29 08:39:17,905: INFO: 1945385980: Accuracy: 0.7862068965517242]\n",
      "[2025-08-29 08:39:17,906: INFO: 1945385980: Precision: 0.7887039239001188]\n",
      "[2025-08-29 08:39:17,907: INFO: 1945385980: Recall: 0.7862068965517242]\n",
      "[2025-08-29 08:39:17,909: INFO: 1945385980: F1: 0.784341065830721]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "25/08/29 08:39:18 ERROR Instrumentation: org.apache.hadoop.fs.UnsupportedFileSystemException: No FileSystem for scheme \"mlflow-artifacts\"\n",
      "\tat org.apache.hadoop.fs.FileSystem.getFileSystemClass(FileSystem.java:3443)\n",
      "\tat org.apache.hadoop.fs.FileSystem.createFileSystem(FileSystem.java:3466)\n",
      "\tat org.apache.hadoop.fs.FileSystem.access$300(FileSystem.java:174)\n",
      "\tat org.apache.hadoop.fs.FileSystem$Cache.getInternal(FileSystem.java:3574)\n",
      "\tat org.apache.hadoop.fs.FileSystem$Cache.get(FileSystem.java:3521)\n",
      "\tat org.apache.hadoop.fs.FileSystem.get(FileSystem.java:540)\n",
      "\tat org.apache.hadoop.fs.Path.getFileSystem(Path.java:365)\n",
      "\tat org.apache.spark.ml.util.FileSystemOverwrite.handleOverwrite(ReadWrite.scala:673)\n",
      "\tat org.apache.spark.ml.util.MLWriter.save(ReadWrite.scala:167)\n",
      "\tat org.apache.spark.ml.PipelineModel$PipelineModelWriter.super$save(Pipeline.scala:344)\n",
      "\tat org.apache.spark.ml.PipelineModel$PipelineModelWriter.$anonfun$save$4(Pipeline.scala:344)\n",
      "\tat org.apache.spark.ml.MLEvents.withSaveInstanceEvent(events.scala:174)\n",
      "\tat org.apache.spark.ml.MLEvents.withSaveInstanceEvent$(events.scala:169)\n",
      "\tat org.apache.spark.ml.util.Instrumentation.withSaveInstanceEvent(Instrumentation.scala:42)\n",
      "\tat org.apache.spark.ml.PipelineModel$PipelineModelWriter.$anonfun$save$3(Pipeline.scala:344)\n",
      "\tat org.apache.spark.ml.PipelineModel$PipelineModelWriter.$anonfun$save$3$adapted(Pipeline.scala:344)\n",
      "\tat org.apache.spark.ml.util.Instrumentation$.$anonfun$instrumented$1(Instrumentation.scala:191)\n",
      "\tat scala.util.Try$.apply(Try.scala:213)\n",
      "\tat org.apache.spark.ml.util.Instrumentation$.instrumented(Instrumentation.scala:191)\n",
      "\tat org.apache.spark.ml.PipelineModel$PipelineModelWriter.save(Pipeline.scala:344)\n",
      "\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n",
      "\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\n",
      "\tat java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n",
      "\tat java.base/java.lang.reflect.Method.invoke(Method.java:566)\n",
      "\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n",
      "\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:374)\n",
      "\tat py4j.Gateway.invoke(Gateway.java:282)\n",
      "\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n",
      "\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n",
      "\tat py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)\n",
      "\tat py4j.ClientServerConnection.run(ClientServerConnection.java:106)\n",
      "\tat java.base/java.lang.Thread.run(Thread.java:829)\n",
      "\n",
      "2025/08/29 08:39:48 WARNING mlflow.models.model: Model logged without a signature and input example. Please set `input_example` parameter when logging the model to auto infer the model signature.\n",
      "Registered model 'Titanic_Pipeline_Exp1' already exists. Creating a new version of this model...\n",
      "2025/08/29 08:39:49 INFO mlflow.store.model_registry.abstract_store: Waiting up to 300 seconds for model version to finish creation. Model name: Titanic_Pipeline_Exp1, version 14\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2025-08-29 08:39:49,080: INFO: 1945385980: Run ID from constants: bbbcb472569b406ca0df4db42b6cec40]\n",
      "[2025-08-29 08:39:49,081: INFO: 1945385980: Experiment ID from constants: 285969590943178826]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Created version '14' of model 'Titanic_Pipeline_Exp1'.\n",
      "2025/08/29 08:39:49 INFO mlflow.tracking._tracking_service.client: 🏃 View run LR_Pipeline at: http://localhost:5000/#/experiments/285969590943178826/runs/bbbcb472569b406ca0df4db42b6cec40.\n",
      "2025/08/29 08:39:49 INFO mlflow.tracking._tracking_service.client: 🧪 View experiment at: http://localhost:5000/#/experiments/285969590943178826.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2025-08-29 08:39:49,170: INFO: 1945385980: MLFlow Model Tracking done successfully.]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_8840/1945385980.py:94: FutureWarning: ``mlflow.tracking.client.MlflowClient.get_latest_versions`` is deprecated since 2.9.0. Model registry stages will be removed in a future major release. To learn more about the deprecation of model registry stages, see our migration guide here: https://mlflow.org/docs/latest/model-registry.html#migrating-from-stages\n",
      "  latest_versions = client.get_latest_versions(self.config.params_experiment_name, stages=[\"None\"])\n",
      "/tmp/ipykernel_8840/1945385980.py:97: FutureWarning: ``mlflow.tracking.client.MlflowClient.transition_model_version_stage`` is deprecated since 2.9.0. Model registry stages will be removed in a future major release. To learn more about the deprecation of model registry stages, see our migration guide here: https://mlflow.org/docs/latest/model-registry.html#migrating-from-stages\n",
      "  client.transition_model_version_stage(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model 'Titanic_Pipeline_Exp1' version 14 moved to Prod not on Staging!\n"
     ]
    }
   ],
   "source": [
    "try:\n",
    "    config = ConfigurationManager()\n",
    "    mlflow_model_tracking_config = config.mlflow_model_management_config()\n",
    "    mlflow_model_tracking_config = MLFlowModelManagement(config=mlflow_model_tracking_config)\n",
    "    mlflow_model_tracking_config.load_model()\n",
    "    mlflow_model_tracking_config.mlflow_model_tracking()\n",
    "except Exception as e:\n",
    "    raise e"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "9d307361",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/home/amit/python/Industrial_AI_project/IITM_MLops_titanic_dataset_github_clone/IITM-MLProject-kaggle-Titanic-dataset'"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%pwd"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "titanic_end_sem",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.20"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
